#+startup: showall inlineimages
#+property: header-args:R :session *R* :family "Concourse 3"

#+begin_src R :results silent
library(ISLR2)
library(leaps)  # For best subset selection
library(glmnet)  # For ridge regression and the lasso
library(pls)   # For principal components regression
#+end_src

* Lab
** Subset Selection Methods
*** Best Subset Selection
Here we apply the best subset selection approach to the =Hitters= data. We notice
that the =Salary= variable is missing for some of the players. We can use the
=is.na()= and =na.omit()= functions to identify and remove rows that have missing
values.

#+begin_src R :results output :exports both
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
#+end_src

#+RESULTS:
: [1] 322  20
: [1] 59
: [1] 263  20

The =regsubsets()= function performs best subset selection, where best is
quantified using RSS.

#+begin_src R :results output :exports both
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
reg.summary
#+end_src

#+RESULTS:
#+begin_example
Subset selection object
Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19)
19 Variables  (and intercept)
           Forced in Forced out
AtBat          FALSE      FALSE
Hits           FALSE      FALSE
HmRun          FALSE      FALSE
Runs           FALSE      FALSE
RBI            FALSE      FALSE
Walks          FALSE      FALSE
Years          FALSE      FALSE
CAtBat         FALSE      FALSE
CHits          FALSE      FALSE
CHmRun         FALSE      FALSE
CRuns          FALSE      FALSE
CRBI           FALSE      FALSE
CWalks         FALSE      FALSE
LeagueN        FALSE      FALSE
DivisionW      FALSE      FALSE
PutOuts        FALSE      FALSE
Assists        FALSE      FALSE
Errors         FALSE      FALSE
NewLeagueN     FALSE      FALSE
1 subsets of each size up to 19
Selection Algorithm: exhaustive
          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
1  ( 1 )  " "   " "  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"
2  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"
3  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"
4  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"
5  ( 1 )  "*"   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"
6  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*"
7  ( 1 )  " "   "*"  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "
8  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   "*"    "*"   " "
9  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*"
10  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*"
11  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*"
12  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*"
13  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*"
14  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*"
15  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    "*"   " "    "*"   "*"
16  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*"
17  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*"
18  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   " "    "*"   "*"
19  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   "*"    "*"   "*"
          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
1  ( 1 )  " "    " "     " "       " "     " "     " "    " "
2  ( 1 )  " "    " "     " "       " "     " "     " "    " "
3  ( 1 )  " "    " "     " "       "*"     " "     " "    " "
4  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "
5  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "
6  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "
7  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "
8  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "
9  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "
10  ( 1 ) "*"    " "     "*"       "*"     "*"     " "    " "
11  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "
12  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "
13  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "
14  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "
15  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "
16  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "
17  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"
18  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"
19  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"
#+end_example

An asterisk indicates that a given variable is included in the model. By
default, =regsubsets()= only reports results up to the best eight-variable
model. But the =nvmax= option can be used in order to return as many variable as
desired.

Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all of the models will help us
decide which model to select. The =regsubsets()= function has a built-in =plot()=
command which can be used to display the selected variables for the best model
with a given number of predictors.

#+begin_src R :results output file graphics :file assets/ch06/subset.svg :exports both :width 4 :height 4
plot(regfit.full, scale = "adjr2")
#+end_src

#+RESULTS:
[[file:assets/ch06/subset.svg]]
** Ridge Regression and the Lasso
We will now perform ridge regression and the lasso in order to predict =Salary= on
the =Hitters= data.

#+begin_src R :results silent
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
#+end_src

*** Ridge Regression
The =glmnet()= function has an =alpha= argument that determines what type of model
is fit. If =alpha=0= then a ridge regression model is fit, and if =alpha=1= then a
lasso model is fit.

By default the =glmnet()= function performs ridge regression for an automatically
selected range of $\lambda$ values. Associated with each value of $\lambda$ is a vector of
ridge regression coefficients.

#+begin_src R :results output :exports both
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
#+end_src

#+RESULTS:
: [1]  20 100

We now split the samples into a training set and a test set in order to estimate
the test error. In general, it would be better to use cross-validation to choose
the tuning paramter $\lambda$.

#+begin_src R :results output file graphics :file assets/ch06/ridge.svg :exports both :width 4 :height 4
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
#+end_src

#+RESULTS:
[[file:assets/ch06/ridge.svg]]

Finally, we refit our ridge regression model on the full dataset, using the $\lambda$
value chose by cross-validation, and examine the coefficient estimates.

#+begin_src R :results output :exports both
bestlam <- cv.out$lambda.min
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
#+end_src

#+RESULTS:
:  (Intercept)        AtBat         Hits        HmRun         Runs          RBI
:  15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105
:        Walks        Years       CAtBat        CHits       CHmRun        CRuns
:   1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224
:         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists
:   0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950
:       Errors   NewLeagueN
:  -1.36092945   9.12487765

As expected, none of the coefficients are zero --- ridge regression does not
perform variable selection.

*** The Lasso
We now ask whether the lasso can yield either a more accurate or a more
interpretable model than ridge regression.

#+begin_src R :results output file graphics :file assets/ch06/lasso.svg :exports both :width 4 :height 4
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
#+end_src

#+RESULTS:
[[file:assets/ch06/lasso.svg]]

The lasso has a substaintial advantage over ridge regression in that the
resulting coefficient estimates are sparse.

#+begin_src R :results output :exports both
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
bestlam <- cv.out$lambda.min
out <- glmnet(x, y, alpha = 1, lambda = grid)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
#+end_src

#+RESULTS:
:   (Intercept)         AtBat          Hits         HmRun          Runs
:    1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000
:           RBI         Walks         Years        CAtBat         CHits
:    0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000
:        CHmRun         CRuns          CRBI        CWalks       LeagueN
:    0.02825013    0.21628385    0.41712537    0.00000000   20.28615023
:     DivisionW       PutOuts       Assists        Errors    NewLeagueN
: -116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000
** PCR and PLS Regression
*** Principal Components Regression
Principal components regression (PCR) can be performed using the =pcr()= function,
which is part of the =pls= library.

#+begin_src R :results output :exports both
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit)
#+end_src

#+RESULTS:
#+begin_example
Data: 	X dimension: 263 19
	Y dimension: 263 1
Fit method: svdpc
Number of components considered: 19

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV             452    351.9    353.2    355.0    352.8    348.4    343.6
adjCV          452    351.6    352.7    354.4    352.1    347.6    342.7
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV       345.5    347.7    349.6     351.4     352.1     353.5     358.2
adjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV        349.7     349.4     339.9     341.6     339.2     339.6
adjCV     348.0     347.7     338.2     339.7     337.2     337.6

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
X         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96
Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75
        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
X         96.28     97.26     97.98     98.65     99.15     99.47     99.75
Salary    46.86     47.76     47.82     47.85     48.10     50.40     50.55
        16 comps  17 comps  18 comps  19 comps
X          99.89     99.97     99.99    100.00
Salary     53.01     53.85     54.61     54.61
#+end_example

The CV score is provided for each possible number of components, ranging from
$M=0$ onwards. One can also plot the cross-validation scores using the
=validationplot()= function.

#+begin_src R :results output file graphics :file assets/ch06/pcr.svg :exports both :width 4 :height 4
validationplot(pcr.fit, val.type = "MSEP")
#+end_src

#+RESULTS:
[[file:assets/ch06/pcr.svg]]

We now perform PCR on the training data and evaluate its test set performance.

#+begin_src R :results output :exports both
set.seed(1)
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
#+end_src

#+RESULTS:
: [1] 142811.8

This test set MSE is competitive with the results obtained using ridge
regression and the lasso. However, the final model is difficult to interpret
because it does not perform any kind of variable selection or even directly
produce coefficient estimates.
*** Partial Least Squares
We implement partial least squares using the =plsr()= function.

#+begin_src R :results output :exports both
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
#+end_src

#+RESULTS:
#+begin_example
Data: 	X dimension: 131 19
	Y dimension: 131 1
Fit method: kernelpls
Number of components considered: 19

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV           428.3    325.5    329.9    328.8    339.0    338.9    340.1
adjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV       339.0    347.1    346.4     343.4     341.5     345.4     356.4
adjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV        348.4     349.1     350.0     344.2     344.5     345.0
adjCV     344.2     345.0     345.9     340.4     340.6     341.1

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
X         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71
Salary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66
        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
X         93.17     96.05     97.08     97.61     97.97     98.70     99.12
Salary    55.95     56.12     56.47     56.68     57.37     57.76     58.08
        16 comps  17 comps  18 comps  19 comps
X          99.61     99.70     99.95    100.00
Salary     58.17     58.49     58.56     58.62
#+end_example

The lowest cross-validation error occurs when only $M=1$ partial least squares
directions are used. We now evaluate the corresponding test set MSE.

#+begin_src R :results output :exports both
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
#+end_src

#+RESULTS:
: [1] 151995.3

The test MSE is slightly higher than ridge regression, the lasso, and PCR.

* Exercises
** 1
*** a
The best subset selection model has the smallest training RSS since it tries
every combination of predictors during training.
*** b
We don't know the answer since having the smallest training RSS does not
guarantee having the smallest test RSS.
*** c
- (i) True.
- (ii) True.
- (iii) False.
- (iv) False.
- (v) False.
** 2
*** a
(iii) less flexible and better predictions because less variance, more bias.
*** b
(iii) same as the lasso.
*** c
(ii) more flexible, less bias and more variance.
** 3
*** a
(iv) Decrease steadily. The training error at $s=0$ is the maximum and will
decrease as we relax the constraint.
*** b
(ii) Decrease initially, and then start increasing in an U-shape. When $s=0$,
the model is underfit to the data and has high test RSS. As we increase $s$, the
model will start fitting well and eventually become overfit to the data.
*** c
(iii) Increase steadily. When $s=0$, the model has almost no variance. As we
increase $s$, the variance will keep increasing and the model becomes highly
dependent on the data.
*** d
(iv) Decrease steadily. The bias of the model when $s=0$ is the highest.
*** e
(v) Remain constant. The irreducible error is model independent.
** 4
*** a
(iv) Increase steadily. The training error when $\lambda=0$ is the lowest.
*** b
(ii) Decrease initially, and then start increasing in an U-shape. The model
overfits to the data in the beginning, then start fitting well and eventually
become underfit to the data as we increase $\lambda$.
*** c
(iv) Decrease steadily. When $\lambda=0$, the coefficients are heavily dependent on
the data, hence high variance. When we increase $\lambda$, the variance will gradually
decrease to 0.
*** d
(iii) Increase steadily. The bias of the model when $\lambda=0$ is the lowest.
*** e
(v) Remain constant. The irreducible error is model independent.
** 5
*** a
Minimize

$$
(y_1 - \hat{\beta_1}x_{11} - \hat{\beta_2}x_{12})^2 + (y_2 - \hat{\beta_2}x_{21} - \hat{\beta_2}x_{22})^2 + \lambda(\hat{\beta_1}^2 + \hat{\beta_2}^2).
$$

** 6
** 7
** 8
*** a
#+begin_src R :results silent
set.seed(1)
X <- rnorm(100)
eps <- rnorm(100)
#+end_src
*** b
#+begin_src R :results silent
beta0 <- 1
beta1 <- 2
beta2 <- 3
beta3 <- 4
Y <- beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
#+end_src
*** c
#+begin_src R :results output :exports both
data.full <- data.frame(y = Y, x = X)
regfit.full <- regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
reg.summary <- summary(regfit.full)
which.min(reg.summary$cp)
which.min(reg.summary$bic)
which.max(reg.summary$adjr2)
#+end_src

#+RESULTS:
: [1] 4
: [1] 3
: [1] 4

#+begin_src R :results output file graphics :file assets/ch06/e8c.svg :exports both :width 4 :height 4
plot(reg.summary$cp, xlab = "Subset Size", ylab = "Cp", pch = 20, type = "l")
points(4, reg.summary$cp[4], pch = 16, col = "red")
#+end_src

#+RESULTS:
[[file:assets/ch06/e8c.svg]]
*** d
*** e
#+begin_src R :results output file graphics :file assets/ch06/e8e.svg :exports both :width 4 :height 4
xmat <- model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
lasso.mod <- cv.glmnet(xmat, Y, alpha = 1)
best.lambda <- lasso.mod$lambda.min
plot(lasso.mod)
#+end_src

#+RESULTS:
[[file:assets/ch06/e8e.svg]]
#+begin_src R :results output :exports both
best.lambda <- lasso.mod$lambda.min
lasso.mod <- glmnet(xmat, Y, alpha = 1)
predict(lasso.mod, s = best.lambda, type = "coefficients")
#+end_src

#+RESULTS:
#+begin_example
11 x 1 sparse Matrix of class "dgCMatrix"
                                s1
(Intercept)            1.168794337
poly(x, 10, raw = T)1  2.164793590
poly(x, 10, raw = T)2  2.639485133
poly(x, 10, raw = T)3  3.800683773
poly(x, 10, raw = T)4  0.041512567
poly(x, 10, raw = T)5  0.014068421
poly(x, 10, raw = T)6  .
poly(x, 10, raw = T)7  0.004039751
poly(x, 10, raw = T)8  .
poly(x, 10, raw = T)9  .
poly(x, 10, raw = T)10 .
#+end_example

The lasso prefers low polynomial coefficients, i.e. $X$, $X^2$, and $X^3$.
** 9
** 10
*** a
#+begin_src R :results silent
set.seed(1)
p <- 20
n <- 1000
x <- matrix(rnorm(n * p), n, p)
beta <- rnorm(p)
beta[2] <- 0
beta[4] <- 0
beta[9] <- 0
eps <- rnorm(p)
y <- x %*% beta + eps
#+end_src
*** b
#+begin_src R :results silent
train <- sample(seq(1000), 100, replace = FALSE)
y.train <- y[train, ]
y.test <- y[-train, ]
x.train <- x[train, ]
x.test <- x[-train, ]
#+end_src
*** c
#+begin_src R :results output file graphics :file assets/ch06/e10c.svg :exports both :width 4 :height 4
regfit.full <- regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
errors.val <- rep(NA, p)
cols <- colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
    coefi <- coef(regfit.full, id = i)
    pred <- as.matrix(x.train[, cols %in% names(coefi)]) %*% coefi[names(coefi) %in% cols]
    errors.val[i] <- mean((y.train - pred)^2)
}
plot(errors.val, ylab = "Train MSE", pch = 19, type = "b")
#+end_src

#+RESULTS:
[[file:assets/ch06/e10c.svg]]
*** d
#+begin_src R :results output file graphics :file assets/ch06/e10d.svg :exports both :width 4 :height 4
errors.val <- rep(NA, p)
for (i in 1:p) {
    coefi <- coef(regfit.full, id = i)
    pred <- as.matrix(x.test[, cols %in% names(coefi)]) %*% coefi[names(coefi) %in% cols]
    errors.val[i] <- mean((y.test - pred)^2)
}
plot(errors.val, ylab = "Test MSE", pch = 19, type = "b")
#+end_src

#+RESULTS:
[[file:assets/ch06/e10d.svg]]
*** e
#+begin_src R :results output :exports both
which.min(errors.val)
#+end_src

#+RESULTS:
: [1] 17

The model with 17 predictors has the smallest MSE, equal to the original data.
*** f
#+begin_src R :results output :exports both
coef(regfit.full, id = 17)
#+end_src

#+RESULTS:
: (Intercept)         x.1         x.3         x.5         x.6         x.7
: -0.06073059  0.17997121 -0.67254439  0.92590972 -0.24629392 -1.39888688
:         x.8        x.10        x.11        x.12        x.13        x.14
:  0.79153372  0.77727294  0.83702056  0.61583171 -0.44087356 -0.84020601
:        x.15        x.16        x.17        x.18        x.19        x.20
: -0.68448585 -0.31047611  0.40764020  1.78899142  0.90020756 -0.97711842

The model is similar to the true underlying model used to generate the data.
